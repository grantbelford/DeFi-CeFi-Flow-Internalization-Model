{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8913e58-64a0-4a45-a2a0-0213f1c9645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step1a. Download Binance Hist Data \n",
    "Snapshot Fn: \n",
    "* Takes OHLCV (1min candles) & order book data\n",
    "* Saves 3 files for each snapshot:\n",
    "1. OHLCV data (open, high, low, close, volume)\n",
    "2. Asks\n",
    "3. Bids \n",
    "\n",
    "Scheduling:\n",
    "* Runs every minute (configurable)\n",
    "* 1st run happens immediately\n",
    "* Runs continuously until stopped with Ctrl+C\n",
    "\n",
    "ex. File Org - Snapshot taken of Binance ETH/USDT pair OHLCV, bids & asks at 14:33pm UTC on 6-May-25. \n",
    "Binance/ETHUSDT_20240506_1433_ohlcv.csv\n",
    "Binance/ETHUSDT_20240506_1433_asks.csv\n",
    "Binance/ETHUSDT_20240506_1433_bids.csv\n",
    "'''\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timezone  # Added timezone to imports\n",
    "import os\n",
    "import schedule\n",
    "\n",
    "# Configuration\n",
    "BINANCE_SYMBOLS = ['ETH/USDT']\n",
    "OUTPUT_DIR = \"Binance\"\n",
    "SNAPSHOT_INTERVAL_MINUTES = 1\n",
    "\n",
    "def initialize_binance_client():\n",
    "    \"\"\"Initialize and return Binance client with rate limiting\"\"\"\n",
    "    return ccxt.binance({\n",
    "        'enableRateLimit': True,\n",
    "        'options': {\n",
    "            'defaultType': 'spot',\n",
    "            'adjustForTimeDifference': True,}})\n",
    "\n",
    "def ensure_output_directory():\n",
    "    \"\"\"Ensure output dir exists\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def take_orderbook_snapshot(binance, symbol):\n",
    "    \"\"\"Take snapshot of order book and OHLCV data for a symbol\"\"\"\n",
    "    try:\n",
    "        base_symbol = symbol.replace('/', '')\n",
    "        current_time = datetime.now(timezone.utc)\n",
    "        timestamp = current_time.strftime(\"%Y%m%d_%H%M\")  \n",
    "        \n",
    "        # Prepare filenames\n",
    "        prefix = f\"{OUTPUT_DIR}/{base_symbol}_{timestamp}\"\n",
    "        ohlcv_file = f\"{prefix}_ohlcv.csv\"\n",
    "        asks_file = f\"{prefix}_asks.csv\"\n",
    "        bids_file = f\"{prefix}_bids.csv\"\n",
    "        \n",
    "        # Skip if files already exist (prevent overwrites within same second)\n",
    "        if os.path.exists(ohlcv_file):\n",
    "            print(f\"Files for {timestamp}Z already exist, skipping...\")\n",
    "            return\n",
    "        \n",
    "        # Get current min OHLCV data\n",
    "        ohlcv = binance.fetch_ohlcv(symbol, '1m', limit=1)\n",
    "        orderbook = binance.fetch_order_book(symbol)\n",
    "        \n",
    "        # Verify OHLCV timestamp aligns with our current time\n",
    "        if ohlcv:\n",
    "            ohlcv_timestamp = ohlcv[0][0]\n",
    "            ohlcv_time = datetime.fromtimestamp(ohlcv_timestamp/1000, timezone.utc)\n",
    "            time_diff = (current_time - ohlcv_time).total_seconds()\n",
    "            \n",
    "            if abs(time_diff) > 60:\n",
    "                print(f\"Warning: OHLCV timestamp differs by {time_diff} seconds from current time\")\n",
    "        \n",
    "        # Save OHLCV data\n",
    "        ohlcv_df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        ohlcv_df.to_csv(ohlcv_file, index=False)\n",
    "        \n",
    "        # Save order book data\n",
    "        pd.DataFrame(orderbook['asks']).to_csv(asks_file, index=False, header=['price', 'amount'])\n",
    "        pd.DataFrame(orderbook['bids']).to_csv(bids_file, index=False, header=['price', 'amount'])\n",
    "        \n",
    "        print(f\"Saved UTC snapshot for {symbol} at {timestamp}Z (OHLCV time: {ohlcv_time.strftime('%Y%m%d_%H%M%S')}Z)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error taking snapshot for {symbol}: {str(e)}\")\n",
    "\n",
    "def job():\n",
    "    \"\"\"Job to run every minute\"\"\"\n",
    "    binance = initialize_binance_client()\n",
    "    for symbol in BINANCE_SYMBOLS:\n",
    "        take_orderbook_snapshot(binance, symbol)\n",
    "\n",
    "def run_scheduler():\n",
    "    \"\"\"Run the scheduler continuously\"\"\"\n",
    "    print(f\"Starting orderbook snapshot service. Snapshots every {SNAPSHOT_INTERVAL_MINUTES} minute(s) in UTC...\")\n",
    "    print(f\"Press Ctrl+C to stop\")\n",
    "    \n",
    "    # Schedule the job\n",
    "    schedule.every(SNAPSHOT_INTERVAL_MINUTES).minutes.do(job)\n",
    "    \n",
    "    # Run immediately first time\n",
    "    job()\n",
    "    \n",
    "    # Run continuously\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ensure_output_directory()\n",
    "    try:\n",
    "        run_scheduler()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping orderbook snapshot service...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4172e2b-b9b3-4429-ad88-4304b78a6f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "COMPREHENSIVE MISSING DATA ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Expected time range: 2025-05-05 08:00:00 to 2025-05-07 10:00:00\n",
      "Total expected minutes: 3001\n",
      "Total expected files: 9003\n",
      "\n",
      "Files found:\n",
      "- OHLCV: 2200\n",
      "- Bids: 2200\n",
      "- Asks: 2200\n",
      "TOTAL: 6600\n",
      "\n",
      "Missing files:\n",
      "- OHLCV: 801\n",
      "- Bids: 801\n",
      "- Asks: 801\n",
      "TOTAL: 2403\n",
      "\n",
      "LONGEST MISSING PERIOD:\n",
      "- Start: 2025-05-06 16:27:00\n",
      "- End: 2025-05-06 16:59:00\n",
      "- Duration: 33.0 minutes (0.6 hours)\n",
      "\n",
      "ALL MISSING PERIODS (>0:01:00):\n",
      "1. 2025-05-06 16:27:00 to 2025-05-06 16:59:00 (33.0 minutes)\n",
      "2. 2025-05-05 17:46:00 to 2025-05-05 18:02:00 (17.0 minutes)\n",
      "3. 2025-05-06 00:41:00 to 2025-05-06 00:57:00 (17.0 minutes)\n",
      "4. 2025-05-06 07:56:00 to 2025-05-06 08:12:00 (17.0 minutes)\n",
      "5. 2025-05-06 12:06:00 to 2025-05-06 12:22:00 (17.0 minutes)\n",
      "6. 2025-05-06 18:27:00 to 2025-05-06 18:43:00 (17.0 minutes)\n",
      "7. 2025-05-07 03:44:00 to 2025-05-07 04:00:00 (17.0 minutes)\n",
      "8. 2025-05-05 18:04:00 to 2025-05-05 18:19:00 (16.0 minutes)\n",
      "9. 2025-05-05 18:37:00 to 2025-05-05 18:52:00 (16.0 minutes)\n",
      "10. 2025-05-05 21:06:00 to 2025-05-05 21:21:00 (16.0 minutes)\n",
      "11. 2025-05-05 21:24:00 to 2025-05-05 21:39:00 (16.0 minutes)\n",
      "12. 2025-05-05 21:41:00 to 2025-05-05 21:56:00 (16.0 minutes)\n",
      "13. 2025-05-05 22:07:00 to 2025-05-05 22:22:00 (16.0 minutes)\n",
      "14. 2025-05-06 15:37:00 to 2025-05-06 15:52:00 (16.0 minutes)\n",
      "15. 2025-05-06 16:10:00 to 2025-05-06 16:25:00 (16.0 minutes)\n",
      "16. 2025-05-06 22:27:00 to 2025-05-06 22:42:00 (16.0 minutes)\n",
      "17. 2025-05-06 22:44:00 to 2025-05-06 22:59:00 (16.0 minutes)\n",
      "18. 2025-05-06 23:01:00 to 2025-05-06 23:16:00 (16.0 minutes)\n",
      "19. 2025-05-07 03:27:00 to 2025-05-07 03:42:00 (16.0 minutes)\n",
      "20. 2025-05-07 04:34:00 to 2025-05-07 04:49:00 (16.0 minutes)\n",
      "21. 2025-05-05 18:21:00 to 2025-05-05 18:35:00 (15.0 minutes)\n",
      "22. 2025-05-05 23:24:00 to 2025-05-05 23:38:00 (15.0 minutes)\n",
      "23. 2025-05-05 23:40:00 to 2025-05-05 23:54:00 (15.0 minutes)\n",
      "24. 2025-05-06 00:08:00 to 2025-05-06 00:22:00 (15.0 minutes)\n",
      "25. 2025-05-06 00:25:00 to 2025-05-06 00:39:00 (15.0 minutes)\n",
      "26. 2025-05-06 01:10:00 to 2025-05-06 01:24:00 (15.0 minutes)\n",
      "27. 2025-05-06 03:53:00 to 2025-05-06 04:07:00 (15.0 minutes)\n",
      "28. 2025-05-06 07:40:00 to 2025-05-06 07:54:00 (15.0 minutes)\n",
      "29. 2025-05-06 11:49:00 to 2025-05-06 12:03:00 (15.0 minutes)\n",
      "30. 2025-05-06 15:54:00 to 2025-05-06 16:08:00 (15.0 minutes)\n",
      "31. 2025-05-06 18:45:00 to 2025-05-06 18:59:00 (15.0 minutes)\n",
      "32. 2025-05-06 19:01:00 to 2025-05-06 19:15:00 (15.0 minutes)\n",
      "33. 2025-05-06 23:28:00 to 2025-05-06 23:42:00 (15.0 minutes)\n",
      "34. 2025-05-06 23:49:00 to 2025-05-07 00:03:00 (15.0 minutes)\n",
      "35. 2025-05-07 04:02:00 to 2025-05-07 04:16:00 (15.0 minutes)\n",
      "36. 2025-05-07 04:18:00 to 2025-05-07 04:32:00 (15.0 minutes)\n",
      "37. 2025-05-07 06:17:00 to 2025-05-07 06:31:00 (15.0 minutes)\n",
      "38. 2025-05-05 17:31:00 to 2025-05-05 17:44:00 (14.0 minutes)\n",
      "39. 2025-05-06 07:25:00 to 2025-05-06 07:38:00 (14.0 minutes)\n",
      "40. 2025-05-06 17:01:00 to 2025-05-06 17:14:00 (14.0 minutes)\n",
      "41. 2025-05-05 23:56:00 to 2025-05-06 00:06:00 (11.0 minutes)\n",
      "42. 2025-05-06 02:06:00 to 2025-05-06 02:16:00 (11.0 minutes)\n",
      "43. 2025-05-07 06:33:00 to 2025-05-07 06:42:00 (10.0 minutes)\n",
      "44. 2025-05-05 20:56:00 to 2025-05-05 21:04:00 (9.0 minutes)\n",
      "45. 2025-05-06 00:59:00 to 2025-05-06 01:07:00 (9.0 minutes)\n",
      "46. 2025-05-06 18:16:00 to 2025-05-06 18:24:00 (9.0 minutes)\n",
      "47. 2025-05-06 23:18:00 to 2025-05-06 23:26:00 (9.0 minutes)\n",
      "48. 2025-05-05 21:58:00 to 2025-05-05 22:05:00 (8.0 minutes)\n",
      "49. 2025-05-06 03:48:00 to 2025-05-06 03:51:00 (4.0 minutes)\n",
      "50. 2025-05-06 22:22:00 to 2025-05-06 22:25:00 (4.0 minutes)\n",
      "51. 2025-05-06 07:21:00 to 2025-05-06 07:23:00 (3.0 minutes)\n",
      "52. 2025-05-06 12:30:00 to 2025-05-06 12:32:00 (3.0 minutes)\n",
      "53. 2025-05-07 04:52:00 to 2025-05-07 04:54:00 (3.0 minutes)\n",
      "54. 2025-05-06 23:46:00 to 2025-05-06 23:47:00 (2.0 minutes)\n",
      "\n",
      "Detailed reports saved to 'analysis/' directory:\n",
      "- missing_files_detailed.csv\n",
      "- missing_periods.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step1b. Above Binance 1 minute csv snaps for hist backtesting purposes encountering robustness issues.\n",
    "'''\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Config\n",
    "DATA_DIR = 'Binance/'\n",
    "START_DATE = datetime(2025, 5, 5, 8, 0)\n",
    "END_DATE = datetime(2025, 5, 7, 10, 0)\n",
    "\n",
    "def analyze_missing_data():\n",
    "    # Generate expected minute intervals\n",
    "    expected_minutes = pd.date_range(START_DATE, END_DATE, freq='1min')\n",
    "    \n",
    "    # Get all existing files\n",
    "    ohlcv_files = set(glob.glob(os.path.join(DATA_DIR, 'ETHUSDT_*_ohlcv.csv')))\n",
    "    bids_files = set(glob.glob(os.path.join(DATA_DIR, 'ETHUSDT_*_bids.csv')))\n",
    "    asks_files = set(glob.glob(os.path.join(DATA_DIR, 'ETHUSDT_*_asks.csv')))\n",
    "    all_files = ohlcv_files | bids_files | asks_files\n",
    "    \n",
    "    # Extract timestamps from filenames\n",
    "    def extract_time(file_set):\n",
    "        times = []\n",
    "        for f in file_set:\n",
    "            try:\n",
    "                parts = os.path.basename(f).split('_')\n",
    "                time_str = f\"{parts[1]}_{parts[2]}\"\n",
    "                dt = datetime.strptime(time_str, '%Y%m%d_%H%M')\n",
    "                times.append(dt)\n",
    "            except:\n",
    "                continue\n",
    "        return pd.Series(times)\n",
    "    \n",
    "    ohlcv_times = extract_time(ohlcv_files)\n",
    "    bids_times = extract_time(bids_files)\n",
    "    asks_times = extract_time(asks_files)\n",
    "    all_times = extract_time(all_files)\n",
    "    \n",
    "    # Find missing minutes for each type\n",
    "    def find_missing(expected, actual):\n",
    "        expected_set = set(expected)\n",
    "        actual_set = set(actual)\n",
    "        return sorted(expected_set - actual_set)\n",
    "    \n",
    "    missing_ohlcv = find_missing(expected_minutes, ohlcv_times)\n",
    "    missing_bids = find_missing(expected_minutes, bids_times)\n",
    "    missing_asks = find_missing(expected_minutes, asks_times)\n",
    "    all_missing = set(missing_ohlcv) | set(missing_bids) | set(missing_asks)\n",
    "    all_missing = sorted(all_missing)\n",
    "    \n",
    "    # Find continuous missing periods (for longest gap analysis)\n",
    "    missing_periods = []\n",
    "    current_start = None\n",
    "    prev_time = None\n",
    "    \n",
    "    for time in all_missing:\n",
    "        if current_start is None:\n",
    "            current_start = time\n",
    "        elif (time - prev_time) > timedelta(minutes=1):\n",
    "            duration = (prev_time - current_start).total_seconds() / 60 + 1\n",
    "            missing_periods.append({\n",
    "                'start': current_start,\n",
    "                'end': prev_time,\n",
    "                'duration_minutes': duration})\n",
    "            current_start = time\n",
    "        prev_time = time\n",
    "    \n",
    "    if current_start is not None:\n",
    "        duration = (prev_time - current_start).total_seconds() / 60 + 1\n",
    "        missing_periods.append({\n",
    "            'start': current_start,\n",
    "            'end': prev_time,\n",
    "            'duration_minutes': duration})\n",
    "    \n",
    "    # Generate report\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"COMPREHENSIVE MISSING DATA ANALYSIS\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"Expected time range: {START_DATE} to {END_DATE}\")\n",
    "    print(f\"Total expected minutes: {len(expected_minutes)}\")\n",
    "    print(f\"Total expected files: {len(expected_minutes)*3}\\n\")\n",
    "    \n",
    "    print(f\"Files found:\")\n",
    "    print(f\"- OHLCV: {len(ohlcv_files)}\")\n",
    "    print(f\"- Bids: {len(bids_files)}\")\n",
    "    print(f\"- Asks: {len(asks_files)}\")\n",
    "    print(f\"TOTAL: {len(all_files)}\\n\")\n",
    "    \n",
    "    print(f\"Missing files:\")\n",
    "    print(f\"- OHLCV: {len(missing_ohlcv)}\")\n",
    "    print(f\"- Bids: {len(missing_bids)}\")\n",
    "    print(f\"- Asks: {len(missing_asks)}\")\n",
    "    print(f\"TOTAL: {len(missing_ohlcv)+len(missing_bids)+len(missing_asks)}\\n\")\n",
    "    \n",
    "    # Longest missing period\n",
    "    if missing_periods:\n",
    "        longest = max(missing_periods, key=lambda x: x['duration_minutes'])\n",
    "        print(f\"LONGEST MISSING PERIOD:\")\n",
    "        print(f\"- Start: {longest['start']}\")\n",
    "        print(f\"- End: {longest['end']}\")\n",
    "        print(f\"- Duration: {longest['duration_minutes']:.1f} minutes ({longest['duration_minutes']/60:.1f} hours)\\n\")\n",
    "        \n",
    "        print(f\"ALL MISSING PERIODS (>{timedelta(minutes=1)}):\")\n",
    "        for i, period in enumerate(sorted(missing_periods, key=lambda x: x['duration_minutes'], reverse=True), 1):\n",
    "            if period['duration_minutes'] > 1:  # Only show gaps >1 minute\n",
    "                print(f\"{i}. {period['start']} to {period['end']} ({period['duration_minutes']:.1f} minutes)\")\n",
    "    else:\n",
    "        print(\"No missing periods found!\")\n",
    "    \n",
    "    # Save detailed reports\n",
    "    os.makedirs('analysis', exist_ok=True)\n",
    "    \n",
    "    # Save missing timestamps\n",
    "    missing_df = pd.DataFrame({\n",
    "        'timestamp': all_missing,\n",
    "        'missing_ohlcv': [1 if t in missing_ohlcv else 0 for t in all_missing],\n",
    "        'missing_bids': [1 if t in missing_bids else 0 for t in all_missing],\n",
    "        'missing_asks': [1 if t in missing_asks else 0 for t in all_missing]})\n",
    "    missing_df.to_csv('analysis/missing_files_detailed.csv', index=False)\n",
    "    \n",
    "    # Save missing periods\n",
    "    if missing_periods:\n",
    "        periods_df = pd.DataFrame(missing_periods)\n",
    "        periods_df = periods_df.sort_values('duration_minutes', ascending=False)\n",
    "        periods_df.to_csv('analysis/missing_periods.csv', index=False)\n",
    "    \n",
    "    print(\"\\nDetailed reports saved to 'analysis/' directory:\")\n",
    "    print(\"- missing_files_detailed.csv\")\n",
    "    print(\"- missing_periods.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_missing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75b738e4-86ba-49fd-8c3d-004737ff2783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1inch data collection...\n",
      "Fetching WETH/USDT trades from Dune since...\n",
      "Successfully loaded 1795 WETH/USDT trades from Dune since 2025-05-05 08:00:00\n",
      "Saved 1inch WETH/USDT data to 1inch/oneinch_weth_usdt_trades.csv\n",
      "Time range: 2025-05-05 08:04:23 to 2025-05-07 09:08:47\n",
      "Total trades: 1795\n",
      "Data collection complete!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step1c.  1inch Fusion WETH/USDT trade data csv download\n",
    "* via Dune Analytics API query.\n",
    "* 1inch Fusion Dune query itself is run first via Dune GUI. \n",
    "* For those wanting to interact w/Dune GUI - query itself is saved for reference as 1inchFusionDuneQuery.pdf\n",
    "Copy the query text off the .pdf into Dune GUI & run.\n",
    ".ipynb code below is then run to save as: \n",
    "\n",
    "Output generated: /1inch/oneinch_weth_usdt_trades.csv \n",
    "Above .csv Will be saved in a zip file along w/Step 1 Binance csv files for those wishing \n",
    "to start directly from Step 2-WETH-USDT.ipynb file. \n",
    "\n",
    "nb. AFAIK unlike Binance OHLCV data, bids & asks orderbook data not hist. accessible (needs to be saved in real-time as done above)\n",
    "'''\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "DUNE_API_KEY = os.getenv('DUNE_API_KEY', '')                                # Enter your DUNE_API_KEY between ''\n",
    "\n",
    "# Enter DUNE_QUERY_ID below after creating & running the query in https://dune.com\n",
    "# DUNE_QUERY_ID will be generated & visible in the (ex. https://dune.com/queries/1234567) \n",
    "DUNE_QUERY_ID = ''                                                          # Enter your DUNE_QUERY_ID between ''    \n",
    "DUNE_API_URL = f\"https://api.dune.com/api/v1/query/{DUNE_QUERY_ID}/results\"\n",
    "#Set as desired\n",
    "START_DATE = datetime(2025, 5, 5, 8, 00)  \n",
    "\n",
    "def load_1inch_data_from_dune():\n",
    "    \"\"\"Load 1inch WETH/USDT trades from Dune Analytics starting from specific timestamp\"\"\"\n",
    "    headers = {'X-Dune-API-Key': DUNE_API_KEY}\n",
    "    params = {'limit': 10000}  # Increased limit to get more data\n",
    "    \n",
    "    try:\n",
    "        print(\"Fetching WETH/USDT trades from Dune since...\")\n",
    "        response = requests.get(DUNE_API_URL, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        if not data.get('result', {}).get('rows'):\n",
    "            raise ValueError(\"No data returned from Dune API\")\n",
    "            \n",
    "        df = pd.DataFrame(data['result']['rows'])\n",
    "        \n",
    "        # Convert & clean data\n",
    "        df['block_time'] = pd.to_datetime(df['block_time'])\n",
    "        df['date'] = df['block_time'].dt.tz_localize(None)\n",
    "        \n",
    "        if 'price_per_eth' not in df.columns:\n",
    "            raise ValueError(\"price_per_eth column missing from Dune query output\")\n",
    "        \n",
    "        # Filter for trades after our start date\n",
    "        df = df[df['date'] >= START_DATE]\n",
    "        \n",
    "        print(f\"Successfully loaded {len(df)} WETH/USDT trades from Dune since {START_DATE}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Dune data: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_data_to_csv():\n",
    "    \"\"\"Main fn to collect & save 1inch data\"\"\"\n",
    "    print(\"Starting 1inch data collection...\")\n",
    "    \n",
    "    # Create dir if it doesn't exist\n",
    "    os.makedirs(\"1inch\", exist_ok=True)\n",
    "    \n",
    "    # Load data from Dune\n",
    "    oneinch_data = load_1inch_data_from_dune()\n",
    "    \n",
    "    # Save to CSV file w/timestamp for versioning\n",
    "    if not oneinch_data.empty:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        #oneinch_file = f\"1inch/oneinch_weth_usdt_trades_{timestamp}.csv\"\n",
    "        oneinch_file = f\"1inch/oneinch_weth_usdt_trades.csv\"\n",
    "        oneinch_data.to_csv(oneinch_file, index=False)\n",
    "        print(f\"Saved 1inch WETH/USDT data to {oneinch_file}\")\n",
    "        print(f\"Time range: {oneinch_data['date'].min()} to {oneinch_data['date'].max()}\")\n",
    "        print(f\"Total trades: {len(oneinch_data)}\")\n",
    "    else:\n",
    "        print(\"No 1inch trade data was retrieved\")\n",
    "    \n",
    "    print(\"Data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_data_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65def36-9720-4265-ac0d-7f03027392f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
